{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98565db6-f5ec-49b1-b802-5aca51a6d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic config\n",
    "MODEL_ID = \"chargoddard/llama3-42b-v0\"\n",
    "USERNAME = \"\"\n",
    "HF_TOKEN = \"\"\n",
    "BASE_PATH = \"/workspace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1171a7-d005-412b-aac1-5706d9109b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install huggingface_hub\n",
    "!apt-get update && apt-get install -y git-lfs make g++ build-essential rsync\n",
    "!bash <(curl -sSL https://g.bodaay.io/hfd) -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0ee9d-b851-4bdd-8023-f340d43838c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup huggingface for upload\n",
    "from huggingface_hub import create_repo, HfApi, ModelCard\n",
    "import os\n",
    "hf_token = HF_TOKEN\n",
    "api = HfApi()\n",
    "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
    "CACHE_DIR = f'{BASE_PATH}/cache'\n",
    "os.environ['HF_HOME'] = CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a981aa4-5bd0-47bd-a13c-2b344f21fb16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download model\n",
    "!mkdir ./models\n",
    "!./hfdownloader -m {MODEL_ID} -s {BASE_PATH}/models -c 3 -q\n",
    "HF_MODEL_FOLDER_NAME=MODEL_ID.replace(\"/\",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b46222-763c-4b6d-bff1-0d5cb32cc3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exllama quant\n",
    "EXLLAMA_BPW = 3.5\n",
    "\n",
    "EXLLAMA_MODEL_NAME = f\"{HF_MODEL_FOLDER_NAME}-{EXLLAMA_BPW:.1f}bpw-EXL2\"\n",
    "EXLLAMA_SAVE_FOLDER = f\"{BASE_PATH}/quants/exl/{EXLLAMA_MODEL_NAME}\"\n",
    "\n",
    "!mkdir -p {EXLLAMA_SAVE_FOLDER}/temp\n",
    "!mkdir -p {EXLLAMA_SAVE_FOLDER}/output\n",
    "\n",
    "!git clone https://github.com/turboderp/exllamav2\n",
    "!pip install -r {BASE_PATH}/exllamav2/requirements.txt\n",
    "!pip install -e {BASE_PATH}/exllamav2\n",
    "\n",
    "!python {BASE_PATH}/exllamav2/util/convert_safetensors.py {BASE_PATH}/models/{HF_MODEL_FOLDER_NAME}/*.bin && rm {BASE_PATH}/models/{HF_MODEL_FOLDER_NAME}/*.bin\n",
    "!python {BASE_PATH}/exllamav2/convert.py -i {BASE_PATH}/models/{HF_MODEL_FOLDER_NAME}/ -o {EXLLAMA_SAVE_FOLDER}/temp -nr -om {EXLLAMA_SAVE_FOLDER}/measurement.json\n",
    "\n",
    "!python {BASE_PATH}/exllamav2/convert.py -i {BASE_PATH}/models/{HF_MODEL_FOLDER_NAME}/ -o {EXLLAMA_SAVE_FOLDER}/temp -nr -m {EXLLAMA_SAVE_FOLDER}/measurement.json -cf {EXLLAMA_SAVE_FOLDER}/output -b {EXLLAMA_BPW}\n",
    "!cp {BASE_PATH}/models/{HF_MODEL_FOLDER_NAME}/README.md {EXLLAMA_SAVE_FOLDER}/output/README.md\n",
    "!cp {EXLLAMA_SAVE_FOLDER}/measurement.json {EXLLAMA_SAVE_FOLDER}/output/measurement.json\n",
    "\n",
    "create_repo(\n",
    "    repo_id = f\"{USERNAME}/{EXLLAMA_MODEL_NAME}\",\n",
    "    repo_type=\"model\",\n",
    "    private=True,\n",
    "    exist_ok=True,\n",
    "    token=hf_token\n",
    ")\n",
    "api.upload_folder(\n",
    "    folder_path=f\"{EXLLAMA_SAVE_FOLDER}/output\",\n",
    "    repo_id=f\"{USERNAME}/{EXLLAMA_MODEL_NAME}\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a19005-8ddc-45f0-ac6e-3f95a3a9f595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exllama re-quant to different BPW with measurement file\n",
    "EXLLAMA_BPW = 3.5 # should match the BPW from the cell above, otherwise the measurement file won't be found\n",
    "NEW_EXLLAMA_BPW = 3.8\n",
    "\n",
    "EXLLAMA_MODEL_NAME = f\"{HF_MODEL_FOLDER_NAME}-{EXLLAMA_BPW:.1f}bpw-EXL2\"\n",
    "NEW_EXLLAMA_MODEL_NAME = f\"{HF_MODEL_FOLDER_NAME}-{NEW_EXLLAMA_BPW:.1f}bpw-EXL2\"\n",
    "\n",
    "EXLLAMA_SAVE_FOLDER = f\"{BASE_PATH}/quants/exl/{EXLLAMA_MODEL_NAME}\"\n",
    "NEW_EXLLAMA_SAVE_FOLDER = f\"{BASE_PATH}/quants/exl/{NEW_EXLLAMA_MODEL_NAME}\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "if not os.path.exists(os.path.join(EXLLAMA_SAVE_FOLDER,\"measurement.json\")):\n",
    "    sys.exit(\"measurement.json not found!\")\n",
    "\n",
    "!mkdir -p {NEW_EXLLAMA_SAVE_FOLDER}/temp\n",
    "!mkdir -p {NEW_EXLLAMA_SAVE_FOLDER}/output\n",
    "\n",
    "!python {BASE_PATH}/exllamav2/convert.py -i {BASE_PATH}/models/{HF_MODEL_FOLDER_NAME}/ -o {NEW_EXLLAMA_SAVE_FOLDER}/temp -nr -m {EXLLAMA_SAVE_FOLDER}/measurement.json -cf {NEW_EXLLAMA_SAVE_FOLDER}/output -b {NEW_EXLLAMA_BPW}\n",
    "!cp {BASE_PATH}/models/{HF_MODEL_FOLDER_NAME}/README.md {NEW_EXLLAMA_SAVE_FOLDER}/output/README.md\n",
    "!cp {EXLLAMA_SAVE_FOLDER}/measurement.json {NEW_EXLLAMA_SAVE_FOLDER}/output/measurement.json\n",
    "\n",
    "create_repo(\n",
    "    repo_id = f\"{USERNAME}/{NEW_EXLLAMA_MODEL_NAME}\",\n",
    "    repo_type=\"model\",\n",
    "    private=True,\n",
    "    exist_ok=True,\n",
    "    token=hf_token\n",
    ")\n",
    "api.upload_folder(\n",
    "    folder_path=f\"{NEW_EXLLAMA_SAVE_FOLDER}/output\",\n",
    "    repo_id=f\"{USERNAME}/{NEW_EXLLAMA_MODEL_NAME}\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab37f7a-90e1-4a3d-93ac-46fde0fdfde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# awq quant\n",
    "!pip install -qqq -U https://github.com/casper-hansen/AutoAWQ/releases/download/v0.2.4/autoawq-0.2.4+cu118-cp310-cp310-linux_x86_64.whl\n",
    "!pip install zstandard\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "AWQ_BITS = 4\n",
    "AWQ_GROUP_SIZE = 128\n",
    "AWQ_VERSION = \"GEMM\"\n",
    "AWQ_ZERO_POINT = True\n",
    "\n",
    "AWQ_QUANT_CONFIG = {\n",
    "    \"w_bit\": AWQ_BITS,\n",
    "    \"q_group_size\": AWQ_GROUP_SIZE,\n",
    "    \"version\": AWQ_VERSION,\n",
    "    \"zero_point\": AWQ_ZERO_POINT\n",
    "}\n",
    "\n",
    "AWQ_MODEL_NAME = f\"{HF_MODEL_FOLDER_NAME}-AWQ\"\n",
    "AWQ_SAVE_FOLDER = f\"{BASE_PATH}/quants/awq/{AWQ_MODEL_NAME}\"\n",
    "\n",
    "# Quantize model\n",
    "AWQ_MODEL = AutoAWQForCausalLM.from_pretrained(AWQ_MODEL_NAME, safetensors=True, low_cpu_mem_usage=True, cache_dir=CACHE_DIR)\n",
    "AWQ_TOKENIZER = AutoTokenizer.from_pretrained(AWQ_MODEL_NAME, trust_remote_code=True, cache_dir=CACHE_DIR)\n",
    "AWQ_MODEL.quantize(AWQ_TOKENIZER, quant_config=AWQ_QUANT_CONFIG)\n",
    "\n",
    "# Save model and tokenizer\n",
    "AWQ_MODEL.save_quantized(AWQ_SAVE_FOLDER)\n",
    "AWQ_TOKENIZER.save_pretrained(AWQ_SAVE_FOLDER)\n",
    "\n",
    "create_repo(\n",
    "    repo_id = f\"{USERNAME}/{AWQ_MODEL_NAME}\",\n",
    "    repo_type=\"model\",\n",
    "    private=True,\n",
    "    exist_ok=True,\n",
    "    token=hf_token\n",
    ")\n",
    "api.upload_folder(\n",
    "    folder_path=AWQ_SAVE_FOLDER,\n",
    "    repo_id=f\"{USERNAME}/{AWQ_MODEL_NAME}\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hqq quant\n",
    "!git clone https://github.com/mobiusml/hqq.git\n",
    "!pip install -e hqq\n",
    "!python hqq/kernels/setup_cuda.py install\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install transformers --upgrade\n",
    "!num_threads=8; OMP_NUM_THREADS=$num_threads CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import torch\n",
    "from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer\n",
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "from hqq.core.quantize import *\n",
    "\n",
    "HQQ_BITS = 2\n",
    "HQQ_GROUP_SIZE = 128\n",
    "\n",
    "HQQ_MODEL_NAME = f\"{HF_MODEL_FOLDER_NAME}-{HQQ_BITS}bit-HQQ\"\n",
    "HQQ_SAVE_FOLDER = f\"{BASE_PATH}/quants/hqq/{HQQ_MODEL_NAME}\"\n",
    "\n",
    "HQQ_QUANT_CONFIG = BaseQuantizeConfig(\n",
    "    nbits=HQQ_BITS,\n",
    "    group_size=HQQ_GROUP_SIZE\n",
    ")\n",
    "\n",
    "HQQ_MODEL = HQQModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    cache_dir=\".\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "HQQ_TOKENIZER = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)\n",
    "HQQ_MODEL.quantize_model(quant_config=HQQ_QUANT_CONFIG, device='cuda')\n",
    "\n",
    "save_folder = MODEL_ID + \"-HQQ\"\n",
    "HQQ_MODEL.save_quantized(HQQ_SAVE_FOLDER)\n",
    "HQQ_TOKENIZER.save_pretrained(HQQ_SAVE_FOLDER)\n",
    "\n",
    "create_repo(\n",
    "    repo_id = f\"{USERNAME}/{HQQ_MODEL_NAME}\",\n",
    "    repo_type=\"model\",\n",
    "    private=True,\n",
    "    exist_ok=True,\n",
    "    token=hf_token\n",
    ")\n",
    "api.upload_folder(\n",
    "    folder_path=HQQ_SAVE_FOLDER,\n",
    "    repo_id=f\"{USERNAME}/{HQQ_MODEL_NAME}\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f319f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gptq quant\n",
    "!pip install --upgrade auto-gptq optimum accelerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "\n",
    "GPTQ_MODEL_NAME = f\"{HF_MODEL_FOLDER_NAME}-GPTQ\"\n",
    "GPTQ_SAVE_FOLDER = f\"{BASE_PATH}/quants/gptq/{GPTQ_MODEL_NAME}\"\n",
    "\n",
    "GPTQ_BITS = 4\n",
    "GPTQ_GROUP_SIZE = 128\n",
    "GPTQ_DAMP_PERCENT = 0.1\n",
    "\n",
    "GPTQ_TOKENIZER = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)\n",
    "GPT_QUANTIZATION_CONFIG = GPTQConfig(bits=GPTQ_BITS, dataset=\"c4\", tokenizer=GPTQ_TOKENIZER, damp_percent=GPTQ_DAMP_PERCENT)\n",
    "GPTQ_MODEL = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\", quantization_config=GPT_QUANTIZATION_CONFIG, low_cpu_mem_usage=True, trust_remote_code=True, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Save model and tokenizer\n",
    "GPTQ_MODEL.save_pretrained(GPTQ_SAVE_FOLDER, use_safetensors=True)\n",
    "GPTQ_TOKENIZER.save_pretrained(GPTQ_SAVE_FOLDER)\n",
    "\n",
    "create_repo(\n",
    "    repo_id = f\"{USERNAME}/{MODEL_NAME}-GPTQ\",\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True,\n",
    "    private=True,\n",
    "    token=hf_token\n",
    ")\n",
    "api.upload_folder(\n",
    "    folder_path=save_folder,\n",
    "    repo_id=f\"{USERNAME}/{MODEL_NAME}-GPTQ\",\n",
    "    token=hf_token\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
